"""
agent.py â€” Agentic RAG: Planlama + AraÅŸtÄ±rma + Cevaplama
=========================================================
Sistemin beyni: soruyu analiz eder, doÄŸru kanunu seÃ§er,
ilgili maddeleri bulur ve kaynaklÄ± cevap Ã¼retir.

OpenAI Function Calling (tool_use) kullanarak hangi kanunun
aranacaÄŸÄ±na LLM'in kendisi karar verir â€” bu yaklaÅŸÄ±ma
"Agentic RAG" denir.

"""

from openai import OpenAI
import json
import re
import mlflow
from src import config
from src.rag_engine import LegalRAGTool
from src import utils

class LegalRAG:
    """
    RAG SÄ°STEMÄ° (ROUTER + RETRIEVER + GENERATOR)
    ---------------------------------------------
    Bu sÄ±nÄ±f sistemin beyni olarak Ã§alÄ±ÅŸÄ±r ve tÃ¼m akÄ±ÅŸÄ± yÃ¶netir:
    1. Planlama (Router): Sorunun hangi kanunla ilgili olduÄŸunu belirler.
    2. AraÅŸtÄ±rma (Retriever): Ä°lgili kanun iÃ§inde vektÃ¶r aramasÄ± yapar.
    3. Cevaplama (Generator): Bulunan bilgileri kullanarak kullanÄ±cÄ±ya cevap Ã¼retir.
    """
    
    def __init__(self):
        """
        Sistemi HazÄ±rla:
        - OpenAI ve ChromaDB baÄŸlantÄ±larÄ±nÄ± kur.
        - MLflow takip sistemini baÅŸlat.
        - TÃ¼m hukuk kaynaklarÄ±nÄ± (Tools) hafÄ±zaya yÃ¼kle.
        """
        self.client = OpenAI(api_key=config.OPENAI_API_KEY)
        self.chroma_client = utils.get_chroma_client()
        
        # MLflow KonfigÃ¼rasyonu
        mlflow.set_tracking_uri(config.MLFLOW_TRACKING_URI)
        mlflow.set_experiment(config.MLFLOW_EXPERIMENT_NAME)
        
        # ARAÃ‡LARI HAZIRLA (Her kanun iÃ§in RAG motoru)
        self.tools_map = {}
        for key, info in config.LEGAL_DOCS.items():
            self.tools_map[key] = LegalRAGTool(info["collection"], self.chroma_client)
            
    def _get_system_prompt(self):
        """
        SÄ°STEM PROMPT (KÄ°MLÄ°K VE KURALLAR)
        ----------------------------------
        AjanÄ±n nasÄ±l davranacaÄŸÄ±nÄ±, hangi Ã¼slubu kullanacaÄŸÄ±nÄ± ve uymasÄ± gereken
        hukuki kurallarÄ± (Normlar HiyerarÅŸisi vb.) burada tanÄ±mlÄ±yoruz.
        """
        return """
        Sen apartman, site ve konut hukuku konusunda uzman bir asistansÄ±n.
        GÃ¶revin: Kat MÃ¼lkiyeti Kanunu, TÃ¼rk BorÃ§lar Kanunu, TÃ¼rk Medeni Kanunu,
        Anayasa ve ilgili yÃ¶netmelikler (asansÃ¶r, yangÄ±n vb.) hakkÄ±ndaki sorularÄ±
        SADECE sana verilen baÄŸlam (context) bilgisine dayanarak cevaplamak.

        CEVAP FORMATI (ZORUNLU):
        - Soruyu doÄŸrudan ve eksiksiz cevapla. Sorulan ÅŸeyin cevabÄ±nÄ± atla geÃ§me.
        - CevabÄ±nÄ± tek paragraf halinde yaz. Uzun aÃ§Ä±klamalar yapma ama soruyu tam karÅŸÄ±la.
        - CevabÄ±na ilgili kanun veya yÃ¶netmeliÄŸin ismiyle baÅŸla.
          Ã–rnekler: "Kat MÃ¼lkiyeti Kanunu uyarÄ±nca, ...", "TÃ¼rk BorÃ§lar Kanunu uyarÄ±nca, ..."
        - Madde numarasÄ± YAZMA. Kaynak referansÄ± sistem tarafÄ±ndan otomatik ekleniyor.
        - BaÅŸlÄ±k, madde iÅŸareti, numara listesi KULLANMA. DÃ¼z metin yaz.

        KRÄ°TÄ°K KURALLAR:
        1. SADECE sana verilen baÄŸlamdaki bilgiyi kullan. BaÄŸlamda olmayan bilgiyi EKLEME.
        2. BaÄŸlamda bilgi yoksa sadece "Bu konuda verilen metinlerde bilgi bulunmamaktadÄ±r." de.
        3. CevabÄ± TÃ¼rkÃ§e yaz.
        """

    def _extract_article_refs(self, sources):
        """
        MADDE NUMARASI Ã‡IKARICI (Hallucination Ã–nleyici)
        ------------------------------------------------
        Ne Yapar: Retrieved chunk'larÄ±n ham metninden regex ile
        madde numaralarÄ±nÄ± Ã§eker ve kaynak adÄ±na gÃ¶re gruplar.

        Girdi: sources â€” [{'content': '...Madde 20...', 'metadata': {'doc_name': 'Kat MÃ¼lkiyeti Kanunu'}}]
        Ã‡Ä±ktÄ±: "ğŸ“Œ Kaynak: Kat MÃ¼lkiyeti Kanunu (Madde 20, 4)"

        Neden: LLM bazen doÄŸru maddeyi bilse de numarayÄ± yanlÄ±ÅŸ yazabilir
        (hallucination). Bu fonksiyon sadece gerÃ§ekten chunk'ta geÃ§en
        madde numaralarÄ±nÄ± kullanÄ±r.
        """
        # Her kaynak dokÃ¼manÄ± iÃ§in bulunan madde numaralarÄ±nÄ± topla
        doc_articles = {}  # {'Kat MÃ¼lkiyeti Kanunu': {20, 4, 25}, ...}

        for src in sources:
            doc_name = src.get("metadata", {}).get("doc_name", "Bilinmiyor")
            content = src.get("content", "")

            # Regex: "Madde 20", "Ek Madde 3", "GeÃ§ici Madde 1" gibi kalÄ±plarÄ± yakala
            # Negatif lookbehind ile "Ek Madde" ve "GeÃ§ici Madde" ayrÄ± yakalanÄ±r
            patterns = re.findall(r'(?:Ek Madde|GeÃ§ici Madde|Madde)\s+(\d+)', content)

            if doc_name not in doc_articles:
                doc_articles[doc_name] = set()
            # Bulunan numaralarÄ± set'e ekle (tekrar Ã¶nleme)
            doc_articles[doc_name].update(patterns)

        # HiÃ§ madde bulunamadÄ±ysa boÅŸ dÃ¶ndÃ¼r
        if not doc_articles or all(len(v) == 0 for v in doc_articles.values()):
            return ""

        # Formatla: "ğŸ“Œ Kaynak: KMK (Madde 4, 20) | TBK (Madde 314)"
        parts = []
        for doc_name, articles in doc_articles.items():
            if articles:
                # Madde numaralarÄ±nÄ± sayÄ±sal sÄ±raya koy
                sorted_articles = sorted(articles, key=int)
                madde_str = ", ".join([f"Madde {a}" for a in sorted_articles])
                parts.append(f"{doc_name} ({madde_str})")
            else:
                parts.append(doc_name)

        return "ğŸ“Œ Kaynak: " + " | ".join(parts)

    def _get_openai_tools(self):
        """
        LLM'e Sunulacak AraÃ§lar (Tools).
        OpenAI Function Calling formatÄ±na uygun olarak tanÄ±mlanÄ±r.
        """
        return [
            {
                "type": "function",
                "function": {
                    "name": f"search_{key}",
                    "description": info['description'],
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {"type": "string", "description": "Aranacak konu"}
                        },
                        "required": ["query"]
                    }
                }
            }
            for key, info in config.LEGAL_DOCS.items()
        ]

    def generate_answer(self, user_query):
        """
        ANA AKIÅ (Main Flow):
        1. Planlama: Soruyu al ve LLM'e gÃ¶nder. Hangi aracÄ± Ã§aÄŸÄ±racaÄŸÄ±na karar versin.
        2. AraÅŸtÄ±rma: EÄŸer araÃ§ Ã§aÄŸÄ±rdÄ±ysa, ilgili kanun iÃ§inde arama yap.
        3. Cevaplama: Bulunan bilgileri LLM'e geri gÃ¶nder ve nihai cevabÄ± Ã¼rettir.
        """
        # Her sorgu iÃ§in bir MLflow Run baÅŸlat (KullanÄ±m takibi iÃ§in)
        # Not: Metin dosyalarÄ± (artifacts) yoÄŸunluk yaratmamasÄ± iÃ§in loglanmÄ±yor, sadece parametreler takip ediliyor.
        run_name = user_query[:50] + "..." if len(user_query) > 50 else user_query
        with mlflow.start_run(run_name=run_name):
            mlflow.log_params({
                "llm_model": config.LLM_MODEL,
                "top_k": config.TOP_K,
                "embedding_model": config.EMBEDDING_MODEL,
                "temperature": config.TEMPERATURE
            })
            
            # --- 1. ADIM: Planlama ---
            messages = [
                {"role": "system", "content": self._get_system_prompt()},
                {"role": "user", "content": user_query}
            ]
            
            response = self.client.chat.completions.create(
                model=config.LLM_MODEL,
                messages=messages,
                tools=self._get_openai_tools(),
                tool_choice="auto",
                temperature=config.TEMPERATURE
            )
            
            msg = response.choices[0].message
            tool_calls = msg.tool_calls
            used_sources = [] # ArtÄ±k dict listesi olacak
            
            # --- 2. ADIM: AraÃ§ KullanÄ±mÄ± (Varsa) ---
            if tool_calls:
                messages.append(msg)
                
                for tool_call in tool_calls:
                    # Fonksiyon isminden hangi kanunu arayacaÄŸÄ±nÄ± anla (Ã¶rn: search_kmk -> kmk)
                    doc_key = tool_call.function.name.replace("search_", "")
                    query = json.loads(tool_call.function.arguments).get("query")
                    
                    rag_tool = self.tools_map.get(doc_key)
                    context_str = "Bilgi bulunamadÄ±."
                    
                    if rag_tool:
                        results = rag_tool.get_context(query)
                        if results:
                            # Context stringini oluÅŸtur (LLM iÃ§in)
                            context_str = "\n".join([r['content'] for r in results])
                            
                            # Ham veriyi sakla (UI ve Eval iÃ§in)
                            # Her bir result zaten {'content': ..., 'metadata': ...} formatÄ±nda
                            used_sources.extend(results)

                    # AracÄ± Ã§alÄ±ÅŸtÄ±r ve sonucunu mesaja ekle
                    messages.append({
                        "tool_call_id": tool_call.id,
                        "role": "tool",
                        "name": tool_call.function.name,
                        "content": context_str
                    })

                # --- 3. ADIM: Nihai Cevap ---
                final_response = self.client.chat.completions.create(
                    model=config.LLM_MODEL,
                    messages=messages,
                    temperature=config.TEMPERATURE
                )
                answer = final_response.choices[0].message.content
            else:
                # AraÃ§ Ã§aÄŸÄ±rmadÄ±ysa doÄŸrudan cevabÄ± dÃ¶ndÃ¼r
                answer = msg.content

            # --- 4. ADIM: Kaynak ReferansÄ±nÄ± Koddan Ekle ---
            # LLM'in madde numarasÄ± uydurmak yerine, chunk'lardan
            # regex ile Ã§ekilen gerÃ§ek madde numaralarÄ±nÄ± sona ekle.
            ref_header = self._extract_article_refs(used_sources)
            if ref_header:
                answer = f"{answer}\n\n{ref_header}"

            return answer, used_sources
